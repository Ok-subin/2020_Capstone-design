{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPV1TBC5++ifIvSbNlqlpqm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ok-subin/OCR-2020_Capstone-Design-/blob/master/EMNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTskg95TSxMC",
        "outputId": "9ad7537a-02bd-4967-d782-15e9aeb4af55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPgIRxMZTA6j",
        "outputId": "84807d22-12e0-4327-c73a-80f3980626ae"
      },
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2y1VomqmTTk5",
        "outputId": "0f047b9a-16c4-4b92-dbff-babb70908473"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.2.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7hBYBetVIWL"
      },
      "source": [
        "!pip uninstall keras\n",
        "!pip install keras==1.2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IdX5JlR1T9oH",
        "outputId": "95e8e617-f7e5-446e-fc14-3523e86753dc"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqeh0ugUDCH"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI4XYteNSvCp",
        "outputId": "d8e0a08e-dbff-485f-d1f2-86044b34a164"
      },
      "source": [
        "import gzip\n",
        "import numpy\n",
        "import pickle\n",
        "import os\n",
        "from keras.models import save_model\n",
        "import argparse\n",
        "import numpy\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "def _read32(bytestream):\n",
        "    dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
        "    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
        "\n",
        "\n",
        "def extract_images(filename):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        magic = _read32(bytestream)\n",
        "        if magic != 2051:\n",
        "            raise ValueError(\n",
        "                'Invalid magic number %d in MNIST image file: %s' %\n",
        "                (magic, filename))\n",
        "        num_images = _read32(bytestream)\n",
        "        rows = _read32(bytestream)\n",
        "        cols = _read32(bytestream)\n",
        "        buf = bytestream.read(rows * cols * num_images)\n",
        "        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
        "        data = data.reshape(num_images, rows, cols, 1)\n",
        "        return data\n",
        "\n",
        "\n",
        "def extract_labels(filename):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        magic = _read32(bytestream)\n",
        "        if magic != 2049:\n",
        "            raise ValueError(\n",
        "              'Invalid magic number %d in MNIST label file: %s' %\n",
        "              (magic, filename))\n",
        "        num_items = _read32(bytestream)\n",
        "        buf = bytestream.read(num_items)\n",
        "        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
        "        return labels\n",
        "\n",
        "\n",
        "def read_emnist(emnist_dir):\n",
        "    TRAIN_IMAGES = emnist_dir+'/emnist-bymerge-train-images-idx3-ubyte.gz'\n",
        "    TRAIN_LABELS = emnist_dir+'/emnist-bymerge-train-labels-idx1-ubyte.gz'\n",
        "    TEST_IMAGES = emnist_dir+'/emnist-bymerge-test-images-idx3-ubyte.gz'\n",
        "    TEST_LABELS = emnist_dir+'/emnist-bymerge-test-labels-idx1-ubyte.gz'\n",
        "    MAPPING = emnist_dir+'/emnist-bymerge-mapping.txt'\n",
        "\n",
        "    train_images = extract_images(TRAIN_IMAGES)\n",
        "    train_labels = extract_labels(TRAIN_LABELS)\n",
        "    test_images = extract_images(TEST_IMAGES)\n",
        "    test_labels = extract_labels(TEST_LABELS)\n",
        "\n",
        "    with open(MAPPING, \"r\") as f:\n",
        "        mapping = f.readlines()\n",
        "        mapping = {str(x.split()[0]): str(x.split()[1]) for x in mapping}\n",
        "\n",
        "    train_images = train_images.astype('float32')\n",
        "    test_images = test_images.astype('float32')\n",
        "\n",
        "    train_images /= 255\n",
        "    test_images /= 255\n",
        "\n",
        "    # Output : (28, 28, 1)\n",
        "    return ((train_images, train_labels), (test_images, test_labels), mapping)\n",
        "\n",
        "\n",
        "def save(model, mapping, model_name):\n",
        "    os.makedirs(os.path.dirname('./modelSave/'), exist_ok=True)\n",
        "    model_yaml_path = './modelSave/model.yaml'\n",
        "    mapping_model_path = './modelSave/model_mapping.p'\n",
        "    model_h5_path = './modelSave/model.h5'\n",
        "\n",
        "    model_yaml = model.to_yaml()\n",
        "    with open(model_yaml_path, \"w\") as yaml_file:\n",
        "        yaml_file.write(model_yaml)\n",
        "\n",
        "    save_model(model, model_h5_path)\n",
        "\n",
        "    model_json = model.to_json()\n",
        "    with open('./modelSave/model.json', 'w') as json_file:\n",
        "      json_file.write(model_json)\n",
        "    model.save_weights(\"./modelSave/model_weight.h5\")\n",
        "\n",
        "    pickle.dump(mapping, open(mapping_model_path, 'wb'))\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "\n",
        "def build_net(training_data, model_name='model', epochs=200):\n",
        "    (x_train, y_train), (x_test, y_test), mapping = training_data\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train)\n",
        "    y_test = np_utils.to_categorical(y_test)\n",
        "    num_classes = y_test.shape[1]\n",
        "\n",
        "    # -----------------------------ㅡ model -----------------------------------------\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(32, 3, 3, input_shape = (1, 28, 28), activation='relu')) \n",
        "    model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(47, activation='softmax')) \n",
        "\n",
        "    # -----------------------------ㅡ model end------------------------------------\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(0.001), metrics=['accuracy'])\n",
        "\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), nb_epoch=epochs, batch_size=100, verbose=1)\n",
        "    print(model.summary())\n",
        "\n",
        "    scores = model.evaluate(x_test, y_test, batch_size = 500, verbose=0)\n",
        "    print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
        "\n",
        "    save(model, mapping, model_name)\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f', '--file', type=str, help='Path to .mat file')\n",
        "    parser.add_argument('--epochs', type=int, default=1, help='Number of epochs to train on')\n",
        "    parser.add_argument('-m', '--model', type=str, help='Model name')\n",
        "    #args = parser.parse_args() --> error 발생\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    training_data = read_emnist(\"./gzip\")\n",
        "    model = build_net(training_data, args.model, epochs=args.epochs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:321: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:634: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:491: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2866: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:73: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2516: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1047: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:658: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2385: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2389: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:740: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:736: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 697932 samples, validate on 116323 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:112: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:117: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:122: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:269: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:281: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "697932/697932 [==============================] - 1625s - loss: 0.6091 - acc: 0.8064 - val_loss: 0.3295 - val_acc: 0.8826\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}